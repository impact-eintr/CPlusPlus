#### 一、多线程（进程）的同步机制

c++编程中最难的部分有哪些，估计绝大多数人都会首先提出来是多线程（进程）编程。为什么多线程编程难呢？一个主要的原因就是多线程的同步。在多线程同步中，（Linux平台Posix NPTL）中主要有三个同步手段：Semaphore（信号灯）、Mutex（互斥体）和Condition Variables（条件变量）。在c++11的标准中，同样也有包含有类似的相关数据类型(std::mutex,std::condition_variable)。 
**由于CPU的基于流水加上指令顺序的重排（乱序）导致多线程的运行的机制在应用根本无法控制**。但在一些具体的业务场景，又要求必须按顺序一步步来。这时候，就必须使用多线程的同步强制线程按要求来工作。但是在使用这些同步机制时，需要编程者清楚操作系统的进程线程调用机制和相关的数据在多线程间的传递进行控制的方式。而这些配合有一个不能准确到位，就可能会引起程序的崩溃。但是需要注意的是，崩溃并不是最可怕的，可怕是程序跑得很好，但是达不到设计的目的。还有更可怕的，程序跑得很好，正常的情况下也符合预期，但是在某个关键点上，程序就出错了。这样的例子，并不少见，一个比一个难以解决。于是，这也成了多线程编程令一般程序员谈之变色的一个主要原因。 
这次聚集于Linux平台下的多线程同步的底层同步实现机制futex.从这个方向上为解决同步问题做一点努力。 

# 介绍

Futex(Fast Userspace Mutexes)做为一种快速同步机制，从linux 2.5.7开始支持，而c++编程中使用的glibc使用NPTL（Native POSIX Thread Library）做为自己的线程库。而NPTL基本实现了POSIX。 
在Linux早期系统中，并没有线程这个概念，线程的概念在Windows平台上应用居多。而Linux主要是使用Clone的方式来产生进程编程，这也是目前好多人仍然把线程叫做轻量级进程的一个缘由。 
最初，Linux中使用LinuxThreads利用进程来模拟用户空间的线程，但是比较不好的，它不符合Posix的标准，都可以理解的是，CPU在同一进程间的线程之间切换的速度要比多个进程间切换的速度要快。当然，它还有其它很多的缺点。这些缺点，导致开发人员对其并不是非常满意，所以后来IBM和REDHAT团队开始了两个新的实现即：NGPT（Next-Generation POSIX Threads）和NPTL，后来由于某些原因，前者不再维护，硕果仅存，就只有NPTL了。 
LinuxThreads目前很少再维护了，NPTL仍然在发展，不过，NPTL目前还有一些小的问题，比如在SMP架构的CPU上有时会有问题。 

## futex诞生之前

在futex诞生之前，linux下的同步机制可以归为两类：**用户态的同步机制** 和 **内核同步机制**。 用户态的同步机制基本上就是利用原子指令实现的spinlock。最简单的实现就是使用一个整型数，0表示未上锁，1表示已上锁。trylock操作就利用原子指令尝试将0改为1：

```c
bool trylock(int lockval) {
    int old;
    atomic { old = lockval; lockval = 1; }  // 如：x86下的xchg指令
    return old == 0;
}
```

无论spinlock事先有没有被上锁，经历trylock之后，它肯定是已经上锁了。所以lock变量一定被置1。而trylock是否成功，取决于spinlock是事先就被上了锁的（old\==1），还是这次trylock上锁的(old\==0)。而使用原子指令则可以避免多个进程同时看到old==0，并且都认为是自己改它改为1的。

spinlock的lock操作则是一个死循环，不断尝试trylock，直到成功。 对于一些很小的临界区，使用spinlock是很高效的。因为trylock失败时，可以预期持有锁的线程（进程）会很快退出临界区（释放锁）。所以死循环的忙等待很可能要比进程挂起等待更高效。



但是spinlock的应用场景有限，对于大的临界区，忙等待则是件很恐怖的事情，特别是当同步机制运用于等待某一事件时（比如服务器工作线程等待客户端发起请求）。所以很多情况下进程挂起等待是很有必要的。



内核提供的同步机制，诸如semaphore、等，其实骨子里也是利用原子指令实现的spinlock，内核在此基础上实现了进程的睡眠与唤醒。 使用这样的锁，能很好的支持进程挂起等待。但是最大的缺点是每次lock与unlock都是一次系统调用，即使没有锁冲突，也必须要通过系统调用进入内核之后才能识别。



理想的同步机制应该是在没有锁冲突的情况下在用户态利用原子指令就解决问题，而需要挂起等待时再使用内核提供的系统调用进行睡眠与唤醒。换句话说，用户态的spinlock在trylock失败时，能不能让进程挂起，并且由持有锁的线程在unlock时将其唤醒？



如果你没有较深入地考虑过这个问题，很可能想当然的认为类似于这样就行了：

```c
void lock(int lockval) {
    while (!trylock(lockval)) {
        // 空窗期。。。
        wait();  // 如：raise(SIGSTOP)
    }
}
```

但是如果这样做的话，检测锁的trylock操作和挂起进程的wait操作之间会存在一个窗口，如果其间lock发生变化（比如锁的持有者释放了锁），调用者将进入不必要的wait，甚至于wait之后再没有人能将它唤醒。



在futex诞生之前，要实现我们理想中的锁会非常别扭。比如可以考虑用sigsuspend系统调用来实现进程挂起：

```c++
class mutex {
private:
  int lockval;
  spinlocked_set<</SPAN>pid_t> waiters;	// 使用spinlock做保护的set
public:
  void lock() {
    pid_t mypid = getpid();
    waiters.insert(mypid);		// 先将自己加入mutex的等待队列
    while (!trylock(lockval)) {   // 再尝试加锁
      // 进程初始化时需要将SIGUSER1 mask掉，并在此时开启
      sigsuspend(MASK_WITHOUT_SIGUSER1);
    }
    waiters.remove(mypid)		 // 上锁成功之后将自己从等待队列移除
  }
  void unlock() {
    lockval = 0;				  // 先释放锁
    pid_t waiter = waiters.first();  // 再检查等待队列
    if (waiter != 0) {			// 如果有人等待，发送SIGUSER1信号将其唤醒
      kill(waiter, SIGUSER1);
    }
  }
}
```



注意，这里的sigsuspend不同于简单的raise(SIGSTOP)之类wait操作。如果unlock时用于唤醒的kill操作先于sigsuspend发生，sigsuspend也一样能被唤醒。

这样的实现有点类似于老版本的phread_cond，应该还是能work的。有些不太爽的地方，比如sigsuspend系统调用是全局的，并不单单考虑某一把锁。也就是说，lockA的unlock可以将等待lockB的进程唤醒。尽管进程被唤醒之后会继续trylock，并不影响正确性；尽管多数情况下lockA.unlock也并不会试图去唤醒等待lockB的进程（除了一些竞争情况下），因为后者很可能并不在lockA的等待队列中。

另一方面，用户态实现的等待队列也不太爽。它对进程的生命周期是无法感知的，很可能进程挂了，pid却还留在队列中（甚至于一段时间之后又有另一个不相干的进程重用了这个pid，以至于它可能会收到莫名其妙的信号）。所以，unlock的时候如果仅仅给队列中的一个进程发信号，很可能唤醒不了任何等待者。保险的做法只能是全部唤醒，从而引发“惊群“现象。不过，如果仅仅用在多线程（同一进程内部）倒也没关系，毕竟多线程不存在某个线程挂掉的情况（如果线程挂掉，整个进程都会挂掉），而对于线程响应信号而主动退出的情况也是可以在主动退出前注意处理一下等待队列清理的问题。



## futex来了

现在看来，要实现我们想要的锁，对内核就有两点需求：

1、支持一种锁粒度的睡眠与唤醒操作；

2、管理进程挂起时的等待队列。 

于是futex就诞生了。



这玩意是贯通内核空间和用户空间的一把利器。看看它的中文意义“快速用户空间互斥体”。意味着，快。 
Futex是怎么突出这个快的呢？在同步的线程间，开辟一段共享内存（mmap）, futex的变量保存在这段共享内存中，当线程进入（退出）同步区域时，先查询共享内存的中futex变量，如果竞争没有出现，就只修改futex状态，不再进入内核空间调用。反之，还得进入内核控制wait(wake).这样，通过一小段共享内存在用户空间的检查，就极大的避免了不应该进入内核的陷阱。自然，在低频率竞争锁时，效率被大大提高了。 
通过上面的分析可以看出，Futex其实是可以看做内核空间和用户空间两种之间的一种过渡状态.



futex主要有futex_wait和futex_wake两个操作：

```c
// 在uaddr指向的这个锁变量上挂起等待（仅当*uaddr==val时）
int futex_wait(int *uaddr, int val);
// 唤醒n个在uaddr指向的锁变量上挂起等待的进程
int futex_wake(int *uaddr, int n);
```

内核会动态维护一个跟uaddr指向的锁变量相关的等待队列。

注意futex_wait的第二个参数，由于用户态trylock与调用futex_wait之间存在一个窗口，其间lockval可能发生变化（比如正好有人unlock了）。所以用户态应该将自己看到的*uaddr的值作为第二个参数传递进去，futex_wait真正将进程挂起之前一定得检查lockval是否发生了变化，并且检查过程跟进程挂起的过程得放在同一个临界区中。如果futex_wait发现lockval发生了变化，则会立即返回，由用户态继续trylock。

futex实现了锁粒度的等待队列，而这个锁却并不需要事先向内核申明。任何时候，用户态调用futex_wait传入一个uaddr，内核就会维护起与之配对的等待队列。

这件事情听上去好像很复杂，实际上却很简单。其实它并不需要为每一个uaddr单独维护一个队列，futex只维护一个总的队列就行了，所有挂起的进程都放在里面。当然，队列中的节点需要能标识出相应进程在等待的是哪一个uaddr。这样，当用户态调用futex_wake时，只需要遍历这个等待队列，把带有相同uaddr的节点所对应的进程唤醒就行了。

作为优化，futex维护的这个等待队列由若干个带spinlock的链表构成。调用futex_wait挂起的进程，通过其uaddr hash到某一个具体的链表上去。这样一方面能分散对等待队列的竞争、另一方面减小单个队列的长度，便于futex_wake时的查找。每个链表各自持有一把spinlock，将"*uaddr和val的比较操作"与"把进程加入队列的操作"保护在一个临界区中。

另一个问题是关于uaddr参数的比较。futex支持多进程，需要考虑同一个物理内存单元在不同进程中的虚拟地址不同的问题。那么不同进程传递进来的uaddr如何判断它们是否相等，就不是简单数值比较的事情。相同的uaddr不一定代表同一个内存，反之亦然。

**两个进程（线程）要想共享同存，无外乎两种方式：**

1. 通过文件映射（映射真实的文件或内存文件、ipc shmem，以及有亲缘关系的进程通过带MAP_SHARED标记的匿名映射共享内存）、

2. 通过匿名内存映射（比如多线程），

这也是进程使用内存的唯二方式。



那么futex就应该支持这两种方式下的uaddr比较。匿名映射下，需要比较uaddr所在的地址空间（mm）和uaddr的值本身；文件映射下，需要比较uaddr所在的文件inode和uaddr在该inode中的偏移。注意，上面提到的内存共享方式中，有一种比较特殊：有亲缘关系的进程通过带MAP_SHARED标记的匿名映射共享内存。这种情况下表面上看使用的是匿名映射，但是内核在暗中却会转成到/dev/zero这个特殊文件的文件映射。若非如此，各个进程的地址空间不同，匿名映射下的uaddr永远不可能被futex认为相等。